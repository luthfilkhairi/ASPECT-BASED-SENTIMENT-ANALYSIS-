# -*- coding: utf-8 -*-
"""Program_Tesis_Luthfil.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1j0b-d8nqaw_SX8UD-5sDcUkbi1rOELz6

# Mount drive
"""

#from google.colab import drive
#drive.mount('/content/drive')

import nltk
import pandas as pd
import re
import tensorflow as tf
import time
import multiprocessing
import io
import gensim
import numpy as np
import matplotlib.pyplot as plt
import keras
import keras_metrics as km

from tensorflow import keras
from keras.models import Sequential
from keras.optimizers import Adam
from keras.callbacks import ModelCheckpoint
from keras.preprocessing.text import Tokenizer
from keras.models import Sequential
from keras.layers import Embedding
from keras.layers import Dense, Activation, Embedding, LSTM, Bidirectional, Dropout, GRU
from keras import regularizers
from keras.utils import to_categorical
from sklearn.naive_bayes import MultinomialNB
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.model_selection import KFold
from sklearn import metrics
from nltk.tokenize import TweetTokenizer
from collections import defaultdict
from datetime import timedelta
from gensim.models import word2vec
from sklearn.metrics import classification_report
from sklearn.metrics import f1_score, precision_score, recall_score, confusion_matrix, accuracy_score
from sklearn.feature_extraction.text import CountVectorizer
from varname import nameof

"""#2.Load Data"""

#simpan path dataset
path_data_aspek = 'dataset_aspek.csv'
path_data_sentimen = 'dataset_sentimen.csv'
 
#read dataset
data_aspek = pd.read_csv(path_data_aspek, sep=";", header=[0], encoding="UTF-8")
data_sentimen = pd.read_csv(path_data_sentimen, sep=";", header=[0], encoding="UTF-8")

"""#3.Implementasi Preprocessing"""

#menyimpan tweet. (tipe data series pandas)
data_content = data_aspek['content']
# print(data_content)

"""##3.1. Casefolding"""

#casefolding
data_casefolding = data_content.str.lower()
data_casefolding.head()

"""##3.2. Filtering"""

#filtering

#url
filtering_url = [re.sub(r'''(?i)\b((?:https?://|www\d{0,3}[.]|[a-z0-9.\-]+[.][a-z]{2,4}/)(?:[^\s()<>]+|\(([^\s()<>]+|(\([^\s()<>]+\)))*\))+(?:\(([^\s()<>]+|(\([^\s()<>]+\)))*\)|[^\s`!()\[\]{};:'".,<>?«»“”‘’]))''', " ", tweet) for tweet in data_casefolding]
#cont
filtering_cont = [re.sub(r'\(cont\)'," ", tweet)for tweet in filtering_url]
#punctuatuion
filtering_punctuation = [re.sub('[!"”#$%&’()*+,-./:;<=>?@[\]^_`{|}~]', ' ', tweet) for tweet in filtering_cont]  #hapus simbol'[!#?,.:";@()-_/\']'
#  hapus #tagger
filtering_tagger = [re.sub(r'#([^\s]+)', '', tweet) for tweet in filtering_punctuation]
#numeric
filtering_numeric = [re.sub(r'\d+', ' ', tweet) for tweet in filtering_tagger]

# # filtering RT , @ dan #
# fungsi_clen_rt = lambda x: re.compile('\#').sub('', re.compile('rt @').sub('@', x, count=1).strip())
# clean = [fungsi_clen_rt for tweet in filtering_numeric]

data_filtering = pd.Series(filtering_numeric)

data_filtering

"""## 3.3. Tokenisasi"""

#tokenize
tknzr = TweetTokenizer()
data_tokenize = [tknzr.tokenize(tweet) for tweet in data_filtering]

data_tokenize[:10]

"""##3.4. Konversi Slangword"""

#slang word
path_dataslang = open('kamus kata baku-clear.csv')
dataslang = pd.read_csv(path_dataslang, encoding = 'utf-8', header=None, sep=";")

def replaceSlang(word):
  if word in list(dataslang[0]):
    indexslang = list(dataslang[0]).index(word)
    return dataslang[1][indexslang]
  else:
    return word

data_formal = []
for data in data_tokenize:
  data_clean = [replaceSlang(word) for word in data]
  data_formal.append(data_clean)
len_data_formal = len(data_formal)
print(data_formal)
len_data_formal

"""## 3.5. Stopword"""

nltk.download('stopwords')
default_stop_words = nltk.corpus.stopwords.words('indonesian')
stopwords = set(default_stop_words)

def removeStopWords(line, stopwords):
  words = []
  for word in line:  
    word=str(word)
    word = word.strip()
    if word not in stopwords and word != "" and word != "&":
      words.append(word)

  return words
reviews = [removeStopWords(line,stopwords) for line in data_formal]

len(stopwords)

reviews

"""#4.Konversi Kalimat"""

#Pembuatan Kamus kata
t  = Tokenizer()
fit_text = reviews
t.fit_on_texts(fit_text)

#Pembuatan Id masing-masing kata
sequences = t.texts_to_sequences(reviews)

#hapus duplikat kata yang muncul
list_set_sequence = [list(dict.fromkeys(seq)) for seq in sequences]

#mencari max length sequence
def FindMaxLength(lst): 
    maxList = max((x) for x in lst) 
    maxLength = max(len(x) for x in lst ) 
    return maxList, maxLength 
      
# Driver Code 
max_seq, max_length_seq = FindMaxLength(list_set_sequence)
jumlah_index = len(t.word_index) +1

print('jumlah index : ',jumlah_index,'\n')
print('word_index : ',t.word_index,'\n')
print('index kalimat asli     : ', sequences,'\n')
print('kalimat tanpa duplikat : ',list_set_sequence,'\n')
print('panjang max kalimat : ', max_length_seq,'kata','\n')
# print('kalimat terpanjang setelah dihapus duplikat : ', max_seq,'\n')

count_word = [len(i) for i in list_set_sequence]
print('list panjang kalimat : ', count_word)
max_len_word = max(count_word)
print(max_len_word)

min_len_word=min(count_word)
print(min_len_word)

print ("Original list is : " + str(list_set_sequence[0]))

"""## 4.1. Padding"""

#Padding
from keras.preprocessing.sequence import pad_sequences
padding= pad_sequences([list(list_set_sequence[i]) for i in range(len(list_set_sequence))], 
                       maxlen= max_len_word, padding='pre')
padding[:20]

"""#5.Word2vec"""

# #Baca Korpus Wikipedia Indonesia

"""#6.Implementasi LSTM (Klasifikasi Aspek)"""

#Panggil Data target aspek (y)
data_aspek_integritas = data_aspek['integritas']
data_aspek_kapabilitas = data_aspek['kapabilitas']
data_aspek_empati = data_aspek['empati']
data_aspek_akseptabilitas = data_aspek['akseptabilitas']
data_aspek_kontinuitas = data_aspek['kontinuitas']

data_sentimen_integritas = data_sentimen['integritas']
data_sentimen_kapabilitas = data_sentimen['kapabilitas']
data_sentimen_empati = data_sentimen['empati']
data_sentimen_akseptabilitas = data_sentimen['akseptabilitas']
data_sentimen_kontinuitas = data_sentimen['kontinuitas']

"""## 6.1.Split dataset"""

X_train, X_test, y_train, y_test = train_test_split(padding, data_aspek_integritas, test_size=0.2)
nama_aspek = nameof(data_aspek_integritas)
y_train_tr = to_categorical(y_train, num_classes=2)
y_test_tr = to_categorical(y_test, num_classes=2)
print (X_train.shape, y_train_tr.shape)
print (X_test.shape, y_test_tr.shape)
data_aspek_integritas

# K-Fold
from sklearn.model_selection import KFold # import KFold
from sklearn.model_selection import StratifiedKFold


kf = StratifiedKFold(n_splits=5)#,shuffle=True, random_state=None) # Define the split - into 5 folds 
# skf = StratifiedKFold(n_splits=3)
kf.get_n_splits(X_train) # returns the number of splitting iterations in the cross-validator
print(kf)

"""##6.3. Embedding Layer (konversi hasil padding menjadi vektor)"""

# implementasi konversi hasil padding menjadi vektor
import gensim
from gensim.models import Word2Vec
from gensim.utils import simple_preprocess
from gensim.models.keyedvectors import KeyedVectors

# KeyedVectors.load_word2vec_format, gensim.models.Word2Vec.load #,binary='True', encoding='utf-8', unicode_errors='ignore'
word_vectors = gensim.models.Word2Vec.load(
    '/content/drive/My Drive/Thesis/Tesis/program/data/model/word_embedding/CBOW/word2vec_cbow_300.model')  

EMBEDDING_DIM=300
vocabulary_size= jumlah_index  #min(len(word_index)+1,NUM_WORDS)
embedding_matrix = np.zeros((vocabulary_size, EMBEDDING_DIM))
for word, i in t.word_index.items():
    try:
        embedding_vector = word_vectors[word]
        embedding_matrix[i] = embedding_vector
    except KeyError:
        embedding_matrix[i]=np.random.normal(0,np.sqrt(0.25),EMBEDDING_DIM)

del(word_vectors)

from keras.layers import Embedding
embedding_layer = Embedding(vocabulary_size,
                            EMBEDDING_DIM,
                            weights=[embedding_matrix],
                            trainable=False)

from keras.layers import Input
input_embedding = Input(shape=(padding.shape))
embedding = embedding_layer(input_embedding)
embedding

embedding_matrix[1]

"""##6.4. LSTM Layer (Arsitektur Model)"""

from sklearn.model_selection import GridSearchCV
from keras.wrappers.scikit_learn import KerasClassifier

def LSTMmodel_aspek(learn_rate,dropout_rate,neurons,regularisasi):
  # defined model
  tf.keras.backend.clear_session()
  model = Sequential()
  model.add(Embedding(input_dim = jumlah_index,
                      output_dim = EMBEDDING_DIM, 
                      weights = [embedding_matrix], 
                      input_length = max_len_word,
                      trainable=False)) # penggunaan trainable = false karena untuk mencegah bobot di update selama pelatihan
  model.add(LSTM(neurons, return_sequences=False))
  model.add(Dropout(dropout_rate))
  model.add(Dense(2, 
                  activation='softmax', 
                  kernel_regularizer=regularizers.l2(regularisasi))) 
  model.summary()
  
  # compile & fit model
  adam = Adam(lr=learn_rate, beta_1=0.9, beta_2=0.999)
  model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])

  return model

model = KerasClassifier(build_fn=LSTMmodel_aspek, verbose=1)
learn_rate = [0.1, 0.01, 0.001, 0.0001]
dropout_rate = [0.2, 0.5]
batch_size = [3, 7, 15, 20, 32, 64]
neurons = [10, 20, 50, 75]
regularisasi = [0.01, 0.001, 0.0001]

param_grid = dict(learn_rate=learn_rate,dropout_rate=dropout_rate,neurons=neurons,regularisasi=regularisasi,batch_size=batch_size, epochs=epochs)
grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=5,scoring='accuracy')
grid_result = grid.fit(X_train, y_train)

# summarize results
print("Best: %f using %s" % (grid_result.best_score_, grid_result.best_params_))
means = grid_result.cv_results_['mean_test_score']
stds = grid_result.cv_results_['std_test_score']
params = grid_result.cv_results_['params']
for mean, stdev, param in zip(means, stds, params):
    print("%f (%f) with: %r" % (mean, stdev, param))
